{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Training our Model\n",
    "We'll first setup the glue context in which we can read the glue data catalog, as well as setup some constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "database_name = '2019reinventWorkshop'\n",
    "canonical_table_name = \"canonical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data using the Catalog\n",
    "Using the glue context, we can read in the data.  This is done by using the glue data catalog and looking up the data\n",
    "\n",
    "Here we can see there are ***500 million*** records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = glueContext.create_dynamic_frame.from_catalog(database=database_name, table_name=canonical_table_name)\n",
    "print(\"2018/2019 Taxi Data Count: \", taxi_data.count())\n",
    "taxi_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching in Spark\n",
    "We'll use the taxi dataframe a bit repeatitively, so we'll cache it ehre and show some sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = taxi_data.toDF().cache()\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing invalid dates\n",
    "When we originally looked at this data, we saw that it had a lot of bad data in it, and timestamps that were outside the range that are valid.  Let's ensure we are only using the valid records when aggregating and creating our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "dates = (\"2018-01-01\",  \"2019-07-01\")\n",
    "date_from, date_to = [to_date(lit(s)).cast(TimestampType()) for s in dates]\n",
    "\n",
    "df  = df.where((df.pickup_datetime > date_from) & (df.pickup_datetime < date_to))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to restructure this so that each time is a single row, and the time series values are in the series, followed by the numerical and categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our time series (from individual records)\n",
    "Right now they are individual records down to the second level, we'll create a record at the day level for each record and then count/aggregate over those.\n",
    "\n",
    "Let's start by adding a ts_resampled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as max_, min as min_\n",
    "\n",
    "## day = seconds*minutes*hours\n",
    "unit = 60 * 60 * 24\n",
    "epoch = (col(\"pickup_datetime\").cast(\"bigint\") / unit).cast(\"bigint\") * unit\n",
    "\n",
    "with_epoch = df.withColumn(\"epoch\", epoch)\n",
    "\n",
    "min_epoch, max_epoch = with_epoch.select(min_(\"epoch\"), max_(\"epoch\")).first()\n",
    "\n",
    "# Reference range \n",
    "ref = spark.range(\n",
    "    min_epoch, max_epoch + 1, unit\n",
    ").toDF(\"epoch\")\n",
    "\n",
    "resampled_df = (ref\n",
    "    .join(with_epoch, \"epoch\", \"left\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .withColumn(\"ts_resampled\", col(\"epoch\").cast(\"timestamp\")))\n",
    "\n",
    "resampled_df.cache()\n",
    "\n",
    "resampled_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our time series data\n",
    "You can see now that we are resampling per day the resample column, in which we can now aggregate across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "count_per_day_resamples = resampled_df.groupBy([\"ts_resampled\", \"type\"]).count()\n",
    "count_per_day_resamples.cache()\n",
    "count_per_day_resamples.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We restructure it so that each taxi type is it's own column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = count_per_day_resamples.groupBy([\"ts_resampled\"])\\\n",
    ".pivot('type')\\\n",
    ".sum(\"count\").cache()\n",
    "\n",
    "time_series_df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Data Manipulation\n",
    "Now that we an aggregated time series that is much smaller -- let's send this back to the local python environment off the spark cluster on Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o time_series_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are in the local panda/python environment now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "print(time_series_df.dtypes)\n",
    "\n",
    "time_series_df = time_series_df.set_index('ts_resampled', drop=True)\n",
    "time_series_df = time_series_df.sort_index()\n",
    "\n",
    "time_series_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll create the training window next,  We are going to predict the next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "## number of time-steps that the model is trained to predict\n",
    "prediction_length = 14\n",
    "\n",
    "n_weeks = 4\n",
    "end_training = time_series_df.index[-n_weeks*prediction_length]\n",
    "print('end training time', end_training)\n",
    "\n",
    "time_series = []\n",
    "for ts in time_series_df.columns:\n",
    "    time_series.append(time_series_df[ts])\n",
    "    \n",
    "time_series_training = []\n",
    "for ts in time_series_df.columns:\n",
    "    time_series_training.append(time_series_df.loc[:end_training][ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll install matplotlib in the local kernel to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "!pip install matplotlib > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training and test dataset:\n",
    "In this next cell, we can see how the training and test datasets are split up.  Since this is time series, we don't do a random split, instead, we look at how far in the future we are predicting and using that a a knob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#cols_float = time_series_df.drop(['pulocationid', 'dolocationid'], axis=1).columns\n",
    "cols_float = time_series_df.columns\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "colors = cmap(np.arange(0,len(cols_float))/len(cols_float))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,8]);\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(time_series_df.loc[:end_training][cols_float[c]], alpha=0.5, color=colors[c], label=cols_float[c]);  \n",
    "plt.legend(loc='center left');\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(time_series_df.loc[end_training:][cols_float[c]], alpha=0.25, color=colors[c], label=None);\n",
    "plt.axvline(x=end_training, color='k', linestyle=':');\n",
    "plt.text(time_series_df.index[int((time_series_df.shape[0]-n_weeks*prediction_length)*0.75)], time_series_df.max().max()/2, 'Train');\n",
    "plt.text(time_series_df.index[time_series_df.shape[0]-int(n_weeks*prediction_length/2)], time_series_df.max().max()/2, 'Test');\n",
    "plt.xlabel('Time');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning our Time Series\n",
    "FHV still has the issue -- the time series drops when the law is in place.\n",
    "\n",
    "we still need to pull in the FHV HV dataset starting in Feb.  This represents the rideshare apps going to a difference licence type under the NYC TLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are running back on spark now\n",
    "fhvhv_data = glueContext.create_dynamic_frame.from_catalog(database=database_name, table_name=\"fhvhv\")\n",
    "fhvhv_df = fhvhv_data.toDF().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's filter the time range just in case we have additional bad records here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_df = fhvhv_df.where((fhvhv_df.pickup_datetime > date_from) & (fhvhv_df.pickup_datetime < date_to)).cache()\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "fhvhv_df = fhvhv_df.withColumn(\"pickup_datetime\", to_timestamp(\"pickup_datetime\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "fhvhv_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first create our rollup column for the time resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as max_, min as min_\n",
    "\n",
    "## day = seconds*minutes*hours\n",
    "unit = 60 * 60 * 24\n",
    "\n",
    "epoch = (col(\"pickup_datetime\").cast(\"bigint\") / unit).cast(\"bigint\") * unit\n",
    "\n",
    "with_epoch = fhvhv_df.withColumn(\"epoch\", epoch)\n",
    "\n",
    "min_epoch, max_epoch = with_epoch.select(min_(\"epoch\"), max_(\"epoch\")).first()\n",
    "\n",
    "ref = spark.range(\n",
    "    min_epoch, max_epoch + 1, unit\n",
    ").toDF(\"epoch\")\n",
    "\n",
    "resampled_fhvhv_df = (ref\n",
    "    .join(with_epoch, \"epoch\", \"left\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .withColumn(\"ts_resampled\", col(\"epoch\").cast(\"timestamp\")))\n",
    "\n",
    "resampled_fhvhv_df = resampled_fhvhv_df.cache()\n",
    "\n",
    "resampled_fhvhv_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Time Series now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "count_per_day_resamples = resampled_fhvhv_df.groupBy([\"ts_resampled\"]).count()\n",
    "count_per_day_resamples.cache()\n",
    "count_per_day_resamples.show(10, False)\n",
    "fhvhv_timeseries_df = count_per_day_resamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we bring this new time series back locally to join it w/ the existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o fhvhv_timeseries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We rename the count column to be fhvhv so we can join it w/ the other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "fhvhv_timeseries_df = fhvhv_timeseries_df.rename(columns={\"count\": \"fhvhv\"})\n",
    "fhvhv_timeseries_df = fhvhv_timeseries_df.set_index('ts_resampled', drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing all the time series data\n",
    "When we look at the FHVHV dataset starting in Feb 1st, you can see the time series looks normal and there isn't a giant drop in the dataset on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.figure(figsize=[14,8]);\n",
    "plt.plot(time_series_df.join(fhvhv_timeseries_df), marker='8', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But now we need to combine the FHV and FHVHV dataset\n",
    "Let's create a new dataset and call it full_fhv meaning both for-hire-vehicles and for-hire-vehicles high volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "full_timeseries = time_series_df.join(fhvhv_timeseries_df)\n",
    "full_timeseries = full_timeseries.fillna(0)\n",
    "full_timeseries['full_fhv'] = full_timeseries['fhv'] + full_timeseries['fhvhv']\n",
    "full_timeseries = full_timeseries.drop(['fhv', 'fhvhv'], axis=1)\n",
    "\n",
    "full_timeseries = full_timeseries.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the joined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.figure(figsize=[14,8]);\n",
    "plt.plot(full_timeseries, marker='8', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the training/test split now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#cols_float = time_series_df.drop(['pulocationid', 'dolocationid'], axis=1).columns\n",
    "cols_float = full_timeseries.columns\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "colors = cmap(np.arange(0,len(cols_float))/len(cols_float))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,8]);\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(full_timeseries.loc[:end_training][cols_float[c]], alpha=0.5, color=colors[c], label=cols_float[c]);  \n",
    "plt.legend(loc='center left');\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(full_timeseries.loc[end_training:][cols_float[c]], alpha=0.25, color=colors[c], label=None);\n",
    "plt.axvline(x=end_training, color='k', linestyle=':');\n",
    "plt.text(full_timeseries.index[int((full_timeseries.shape[0]-n_weeks*prediction_length)*0.75)], full_timeseries.max().max()/2, 'Train');\n",
    "plt.text(full_timeseries.index[full_timeseries.shape[0]-int(n_weeks*prediction_length/2)], full_timeseries.max().max()/2, 'Test');\n",
    "plt.xlabel('Time');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "end_training = full_timeseries.index[-n_weeks*prediction_length]\n",
    "print('end training time', end_training)\n",
    "\n",
    "time_series = []\n",
    "for ts in full_timeseries.columns:\n",
    "    time_series.append(full_timeseries[ts])\n",
    "    \n",
    "time_series_training = []\n",
    "for ts in full_timeseries.columns:\n",
    "    time_series_training.append(full_timeseries.loc[:end_training][ts])\n",
    "\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "key_prefix = '2019workshop-deepar/'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "data = ''\n",
    "\n",
    "for ts in time_series_training:\n",
    "    data = data + series_to_jsonline(ts)\n",
    "    data = data + '\\n'\n",
    "    \n",
    "s3_client.put_object(Body=data.encode(encoding), Bucket=bucket, Key=key_prefix + 'data/train/train.json')\n",
    "    \n",
    "\n",
    "data = ''\n",
    "for ts in time_series:\n",
    "    data = data + series_to_jsonline(ts)\n",
    "    data = data + '\\n'\n",
    "\n",
    "s3_client.put_object(Body=data.encode(encoding), Bucket=bucket, Key=key_prefix + 'data/test/test.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting our data and output locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}data\".format(bucket, key_prefix)\n",
    "s3_output_path = \"{}/{}output\".format(bucket, key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the DeepAR Algorithm settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='DeepAR-forecast-taxidata',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")\n",
    "\n",
    "## context_length = The number of time-points that the model gets to see before making the prediction.\n",
    "context_length = 14\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": \"D\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"100\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kicking off the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "estimator.fit(inputs={\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR Deep Dive\n",
    "\n",
    "While the training is happening, Let’s elaborate on the DeepAR model's architecture by walking through an example. When interested in quantifying the confidence of the estimates produced, then it's probabilistic forecasts that are wanted. The data we’re working with is real-valued, so let’s opt for the Gaussian likelihood:\n",
    "$$\\ell(y_t|\\mu_t,\\sigma_t)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\frac{-(y_t-\\mu_t)^2}{2\\sigma^2}}.$$\n",
    "\n",
    "$\\theta$ represents the `parameters of the likelihood`. In the case of Gaussian, $\\theta_t$ will represent the mean and standard deviation:  $$\\theta_t = \\{\\mu_{t},\\sigma_{t}\\}.$$\n",
    "\n",
    "The neural network’s last hidden layer results in $h_{d,t}$. This $h_{d,t}$ will undergo 1 activation function per likelihood parameter. For example, for the Gaussian likelihood, $h_{d,t}$ is transformed by an affine activation function to get the mean:\n",
    "$$\\mu_{t} = w_{\\mu}^T h_{d,t} + b_{\\mu},$$\n",
    "and then $h$ is transformed by a softplus activation to get the standard deviation:\n",
    "$$\\sigma_t = \\log\\left(1 + \\exp(w_{\\sigma}^T h_{d,t} + b_{\\sigma})\\right).$$\n",
    "\n",
    "The `activation parameters` are the $w_{\\mu},b_{\\mu},w_{\\sigma},b_{\\sigma}$ parameters within the activation functions. The NN is trained to learn the fixed constants of the activation parameters.  Since the $h_{d,t}$ output  vary given each time-step's input, this still allows the likelihood parameters to vary over time, and therefore capture dynamic behaviors in the time-series data.\n",
    "\n",
    "![DeepAR Training](images/deepar_training.png)\n",
    "\n",
    "From the above diagram, the <span style=\"color:green\">green</span> input at each time-step is the data point preceding the current time-step’s data, as well as the previous network’s output. For simplicity, on this diagram we aren’t showing covariates which would also be input.\n",
    "\n",
    "The LSTM layers are shown in <span style=\"color:red\">red</span>, and the final hidden layer produces the $h_{d,t}$ value, which we saw in the previous slide will undergo an activation function for each parameter of the specified likelihood. To learn the activation function parameters, the NN takes the $h_{d,t}$ at time $t$ and the data up until time $t$, and performs Stochastic Gradient Descent (SGD) to yield the activation parameters which maximize the likelihood at time $t$. The <span style=\"color:blue\">blue</span> output layer uses the SGD-optimized activation functions to output the maximum likelihood parameters.\n",
    "\n",
    "This is how DeepAR trains its model to your data input. Now we want to DeepAR to give us probabilistic forecasts for the next time-step.\n",
    "\n",
    "![DeepAR Forecast](images/deepar_forecast.png)\n",
    "\n",
    "The <span style=\"color:magenta\">pink</span> line marks our current point in time, divides our training data from data not yet seen. For the first input, it can use the data point of the current time. The input will be processed by the trained LSTM layers, and subsequently get activated by the optimized activation functions to output the maximum-likelihood theta parameters at time $t+1$. \n",
    "\n",
    "Now that DeepAR has completed the likelihood with its parameter estimates, DeepAR can simulate `Monte Carlo (MC) samples` from this likelihood and produce an empirical distribution for the predicted datapoint - the probabilistic forecasts shown in <span style=\"color:purple\">purple</span>. The MC samples produced at time $t+1$ are used as input for time $t+2$, etc, until the end of the prediction horizon. In the interactive plots below, we'll see how Monte Carlo samples are able to provide us a confidence interval about the point estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a realtime predictor\n",
    "\n",
    "### Next we will deploy a predictor,  this may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Predictions on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "ABB = full_timeseries.asfreq('d')\n",
    "print('Green Rides:')\n",
    "print(predictor.predict(ts=ABB.loc[end_training:, 'green'], quantiles=[0.10, 0.5, 0.90], num_samples=100).head())\n",
    "print('\\nYellow Rides:')\n",
    "print(predictor.predict(ts=ABB.loc[end_training:, 'yellow'], quantiles=[0.10, 0.5, 0.90], num_samples=100).head())\n",
    "print('\\nFHV Rides:')\n",
    "print(predictor.predict(ts=ABB.loc[end_training:, 'full_fhv'], quantiles=[0.10, 0.5, 0.90], num_samples=100).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "endpoint_name = predictor.endpoint\n",
    "\n",
    "%store ABB\n",
    "%store endpoint_name\n",
    "%store end_training\n",
    "%store prediction_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll show you in the next notebook, how to recreate the predictor and evaluate the results more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
