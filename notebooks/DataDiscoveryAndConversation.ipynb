{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discover and Transformation\n",
    "in this section of the lab, we'll use Glue to discover new transportation data.  From there, we'll use Athena to query and start looking into the dataset to understand the data we are dealing with.\n",
    "\n",
    "We've also setup a set of ETLs using Glue to create the fields into a canonical form, since all the fields call names different things.  \n",
    "\n",
    "After understanding the data, and cleaning it a little, we'll go into another notebook to perform feature engineering and time series modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Databases and Tables in Glue:\n",
    "When you define a table in the AWS Glue Data Catalog, you add it to a database. A database is used to organize tables in AWS Glue. You can organize your tables using a crawler or using the AWS Glue console. A table can be in only one database at a time.\n",
    "\n",
    "Your database can contain tables that define data from many different data stores.\n",
    "\n",
    "A table in the AWS Glue Data Catalog is the metadata definition that represents the data in a data store. You create tables when you run a crawler, or you can create a table manually in the AWS Glue console. The Tables list in the AWS Glue console displays values of your table's metadata. You use table definitions to specify sources and targets when you create ETL (extract, transform, and load) jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "database_name = '2019reinventWorkshop'\n",
    "\n",
    "## lets first create a namespace for the tables:\n",
    "glue_client = boto3.client('glue')\n",
    "create_database_resp = glue_client.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': database_name,\n",
    "        'Description': 'This database will contain the tables discovered through both crawling and the ETL processes'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a new database, or namespace, that can hold the collection of tables\n",
    "\n",
    "https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases\n",
    "\n",
    "![create db response](images/createdatabaseresponse.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a crawler to populate the AWS Glue Data Catalog with tables. This is the primary method used by most AWS Glue users. A crawler can crawl multiple data stores in a single run. Upon completion, the crawler creates or updates one or more tables in your Data Catalog. Extract, transform, and load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. The ETL job reads from and writes to the data stores that are specified in the source and target Data Catalog tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_name = '2019reinventworkshopcrawler'\n",
    "create_crawler_resp = glue_client.create_crawler(\n",
    "    Name=crawler_name,\n",
    "    Role='GlueRole',\n",
    "    DatabaseName=database_name,\n",
    "    Description='Crawler to discover the base tables for the workshop',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': 's3://serverless-analytics/reinvent-2019/taxi_data/',\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "response = glue_client.start_crawler(\n",
    "    Name=crawler_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting the crawler, you can go to the glue console if you'd like to see it running.\n",
    "\n",
    "https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers\n",
    "![startcrawlerui](images/startcrawlerui.png \"\")\n",
    "\n",
    "After it finishes crawling, you can see the datasets (represeted as \"tables\") it automatically discovered.\n",
    "![crawler_discovered](images/crawler_discovered.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for the Crawler to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "response = glue_client.get_crawler(\n",
    "    Name=crawler_name\n",
    ")\n",
    "while (response['Crawler']['State'] == 'RUNNING') | (response['Crawler']['State'] == 'STOPPING'):\n",
    "    print(response['Crawler']['State'])\n",
    "    # Wait for 40 seconds\n",
    "    time.sleep(40)\n",
    "    \n",
    "    response = glue_client.get_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "\n",
    "print('finished running', response['Crawler']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the data\n",
    "\n",
    "We'll use Athena to query the data.  Athena allows us to perform SQL queries against datasets on S3, without having to transform them, load them into a traditional sql datastore, and allows rapid ad-hoc investigation.  \n",
    "\n",
    "Later we'll use Spark to do ETL and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip > /dev/null\n",
    "!pip install PyAthena > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena uses S3 to store results to allow different types of clients to read it and so you can go back and see the results of previous queries.  We can set that up next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "athena_data_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create an Athena connection we can use, much like a standard JDBC/ODBC connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "conn = connect(s3_staging_dir=\"s3://\" + athena_data_bucket,\n",
    "               region_name=sagemaker_session.boto_region_name)\n",
    "\n",
    "df = pd.read_sql('SELECT \\'yellow\\' type, count(*) ride_count FROM \"' + database_name + '\".\"yellow\" ' + \n",
    "                 'UNION ALL SELECT \\'green\\' type, count(*) ride_count FROM \"' + database_name + '\".\"green\"' +\n",
    "                 'UNION ALL SELECT \\'fhv\\' type, count(*) ride_count FROM \"' + database_name + '\".\"fhv\"', conn)\n",
    "print(df)\n",
    "df.plot.bar(x='type', y='ride_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_etl = '2019reinvent_green'\n",
    "\n",
    "response = glue_client.start_job_run(\n",
    "    JobName=green_etl,\n",
    "    WorkerType='Standard', # other options include: 'G.1X'|'G.2X',\n",
    "    NumberOfWorkers=5\n",
    ")\n",
    "print('response from starting green')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After kicking it off, you can see it running in the console too:\n",
    "https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs\n",
    "<img src=\"images/ETLStart.png\"/>\n",
    "\n",
    "<b>WAIT UNTIL THE ETL JOB FINISHES BEFORE CONTINUING!</b>\n",
    "<b>ALSO, YOU MUST CHANGE THE BUCKET PATH IN THIS CELL - FIND THE BUCKET IN S3 THAT CONTAINS '2019reinventetlbucket' in the name</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's list the s3 bucket name:\n",
    "!aws s3 ls | grep '2019reinventetlbucket' | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax should be s3://...\n",
    "normalized_bucket = 's3://FILL_IN_BUCKET_NAME'\n",
    "\n",
    "assert(normalized_bucket != 's3://FILL_IN_BUCKET_NAME')\n",
    "assert(normalized_bucket.startswith( 's3://' ))\n",
    "\n",
    "create_crawler_resp = glue_client.create_crawler(\n",
    "    Name=crawler_name + '_normalized',\n",
    "    Role='GlueRole',\n",
    "    DatabaseName=database_name,\n",
    "    Description='Crawler to discover the base tables for the workshop',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': normalized_bucket + \"/canonical/\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "response = glue_client.start_crawler(\n",
    "    Name=crawler_name + '_normalized'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's wait for the next crawler to finish, this will discover the normalized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "response = glue_client.get_crawler(\n",
    "    Name=crawler_name + '_normalized'\n",
    ")\n",
    "while (response['Crawler']['State'] == 'RUNNING') | (response['Crawler']['State'] == 'STOPPING'):\n",
    "    print(response['Crawler']['State'])\n",
    "    # Wait for 40 seconds\n",
    "    time.sleep(40)\n",
    "    \n",
    "    response = glue_client.get_crawler(\n",
    "        Name=crawler_name + '_normalized'\n",
    "    )\n",
    "\n",
    "print('finished running', response['Crawler']['State'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Normalized Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the total counts for the aggregated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df = pd.read_sql('SELECT type, count(*) ride_count FROM \"' + database_name + '\".\"canonical\" group by type', conn)\n",
    "print(normalized_df)\n",
    "normalized_df.plot.bar(x='type', y='ride_count')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select type, date_trunc('day', pickup_datetime) date, count(*) cnt from \\\"\" + database_name + \"\\\".canonical where pickup_datetime < timestamp '2099-12-31' group by type, date_trunc(\\'day\\', pickup_datetime) \"\n",
    "typeperday_df = pd.read_sql(query, conn)\n",
    "typeperday_df.plot(x='date', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We see some bad data here...\n",
    "We are expecting only 2018 and 2019 datasets here, but can see there are records far into the future and in the past.  This represents bad data that we want to eliminate before we build our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only reason we put this conditional here is so you can execute the cell multiple times\n",
    "# if you don't check, it won't find the 'date' column again and makes interacting w/ the notebook more seemless\n",
    "if type(typeperday_df.index) != pd.core.indexes.datetimes.DatetimeIndex:\n",
    "    print('setting index to date')\n",
    "    typeperday_df = typeperday_df.set_index('date', drop=True)\n",
    "    \n",
    "typeperday_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeperday_df.loc['2018-01-01':'2019-12-31'].plot(y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the bad data now:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the bad data, at least the bad data in the future, is coming from the yellow taxi license type.\n",
    "\n",
    "### Note, we are querying the transformed data.\n",
    "\n",
    "We should check the raw dataset to see if it's also bad or something happened in the ETL process\n",
    "\n",
    "Let's find the two 2088 records to make sure they are in the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"select * from \\\"\" + database_name + \"\\\".yellow where tpep_pickup_datetime like '2088%'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next let's plot this per type:\n",
    "typeperday_df.loc['2018-01-01':'2019-07-30'].pivot_table(index='date', \n",
    "                                                         columns='type', \n",
    "                                                         values='cnt', \n",
    "                                                         aggfunc='sum').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing our Time Series data\n",
    "\n",
    "Some details of what caused this drop:\n",
    "#### On August 14, 2018, Mayor de Blasio signed Local Law 149 of 2018, creating a new license category for TLC-licensed FHV businesses that currently dispatch or plan to dispatch more than 10,000 FHV trips in New York City per day under a single brand, trade, or operating name, referred to as High-Volume For-Hire Services (HVFHS). This law went into effect on Feb 1, 2019\n",
    "\n",
    "Let's bring the other license type and see how it affects the time series charts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_crawler_resp = glue_client.create_crawler(\n",
    "    Name=crawler_name + '_fhvhv',\n",
    "    Role='GlueRole',\n",
    "    DatabaseName=database_name,\n",
    "    Description='Crawler to discover the base tables for the workshop',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': 's3://serverless-analytics/reinvent-2019_moredata/taxi_data/fhvhv/',\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "response = glue_client.start_crawler(\n",
    "    Name=crawler_name + '_fhvhv'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait to discover the fhvhv dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "response = glue_client.get_crawler(\n",
    "    Name=crawler_name + '_fhvhv'\n",
    ")\n",
    "while (response['Crawler']['State'] == 'RUNNING') | (response['Crawler']['State'] == 'STOPPING'):\n",
    "    print(response['Crawler']['State'])\n",
    "    # Wait for 40 seconds\n",
    "    time.sleep(40)\n",
    "    \n",
    "    response = glue_client.get_crawler(\n",
    "        Name=crawler_name + '_fhvhv'\n",
    "    )\n",
    "\n",
    "print('finished running', response['Crawler']['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'select \\'fhvhv\\' as type, date_trunc(\\'day\\', cast(pickup_datetime as timestamp)) date, count(*) cnt from \"' + database_name + '\".\"fhvhv\" group by date_trunc(\\'day\\',  cast(pickup_datetime as timestamp)) '\n",
    "typeperday_fhvhv_df = pd.read_sql(query, conn)\n",
    "typeperday_fhvhv_df = typeperday_fhvhv_df.set_index('date', drop=True)\n",
    "print(typeperday_fhvhv_df.head())\n",
    "typeperday_fhvhv_df.plot(y='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([typeperday_fhvhv_df, typeperday_df], sort=False).loc['2018-01-01':'2019-07-30'].pivot_table(index='date', \n",
    "                                                         columns='type', \n",
    "                                                         values='cnt', \n",
    "                                                         aggfunc='sum').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That looks better -- let's start looking at performing EDA now. Please open the other notebook file in your SageMaker notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
